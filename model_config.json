{
    "block_size": 512,
    "vocab_size": 16384,
    "n_layer": 16,
    "n_head": 16,
    "n_embd": 512,
    "dropout": 0.0,
    "bias": false,
    "use_rope": true,
    "norm_eps": 1e-5,
    "is_causal": true
}